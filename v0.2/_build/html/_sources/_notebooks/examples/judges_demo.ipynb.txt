{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content\n",
    "1. [Introduction](#section1')\n",
    "2. [Set up LLM and Prompts](#section2')\n",
    "3. [Generate Responses and Confidence Scores](#section3')<br>\n",
    "4. [Performance Evaluation](#section4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 18:27:05.041186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741804025.064090   34498 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741804025.070825   34498 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 18:27:05.093362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from uqlm.judges import LLMJudge\n",
    "from uqlm.quantifiers import LLMPanel\n",
    "from uqlm.utils import load_example_dataset, math_postprocessor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "LLM-as-a-Judge methods use one or more LLMs to evaluate the reliability of the original LLM's response. They offer high customizability through prompt engineering and the choice of judge LLM(s).\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "*   ✅  **Highly Customizable:**  Use any LLM as a judge and tailor instruction prompts for specific use cases.\n",
    "*   ✅  **Intuitive:** Leverages the reasoning capabilities of LLMs for evaluation.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "*   ⚠️  **Added Cost:** Requires additional LLM calls for the judge LLM(s).\n",
    "\n",
    "**Available Scorers:**\n",
    "\n",
    "*   Categorical LLM-as-a-Judge ([Manakul et al., 2023](https://arxiv.org/abs/2303.08896); [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175); [Luo et al., 2023](https://arxiv.org/pdf/2303.15621))\n",
    "*   Continuous LLM-as-a-Judge ([Xiong et al., 2024](https://arxiv.org/pdf/2306.13063))\n",
    "*   Panel of LLM Judges ([Verga et al., 2024](https://arxiv.org/abs/2404.18796))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - svamp...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are 87 oranges and 290 bananas in Philip...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marco and his dad went strawberry picking. Mar...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Edward spent $ 6 to buy 2 books each book cost...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frank was reading through his favorite book. T...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There were 78 dollars in Olivia's wallet. She ...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer\n",
       "0  There are 87 oranges and 290 bananas in Philip...    145\n",
       "1  Marco and his dad went strawberry picking. Mar...     19\n",
       "2  Edward spent $ 6 to buy 2 books each book cost...      3\n",
       "3  Frank was reading through his favorite book. T...    198\n",
       "4  There were 78 dollars in Olivia's wallet. She ...     63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (SVAMP)\n",
    "svamp = (\n",
    "    load_example_dataset(\"svamp\")\n",
    "    .rename(columns={\"question_concat\": \"question\", \"Answer\": \"answer\"})[\n",
    "        [\"question\", \"answer\"]\n",
    "    ]\n",
    "    .head(20)\n",
    ")\n",
    "svamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "MATH_INSTRUCTION = (\n",
    "    \"When you solve this math problem only return the answer with no additional text.\\n\"\n",
    ")\n",
    "prompts = [MATH_INSTRUCTION + prompt for prompt in svamp.question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User to populate .env file with API credentials\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "original_llm = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    openai_api_key=os.getenv(\"API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"API_BASE\"),\n",
    "    openai_api_type=os.getenv(\"API_TYPE\"),\n",
    "    openai_api_version=os.getenv(\"API_VERSION\"),\n",
    "    temperature=1,  # User to set temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate Gemini models\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "gemini_pro = ChatVertexAI(model_name=\"gemini-pro\")\n",
    "gemini_flash = ChatVertexAI(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although we use `ChatVertexAI` and `AzureChatOpenAI` in this example, any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Generate responses and confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up credentials and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `LLMJudge()` - Customizable class for LLM-as-a-Judge offering three off-the-shelf templates.\n",
    "\n",
    "**Class Attributes:**\n",
    "- `llm` (**BaseChatModel, default=None**) A langchain llm (`BaseChatModel`). User is responsible for specifying temperature and other relevant parameters to the constructor of their `BaseChatModel` object.\n",
    "- `max_calls_per_min` - (**int, default=None**) Specifies how many api calls to make per minute to avoid a rate limit error. By default, no limit is specified.\n",
    "- `scoring_template` - (**{'true_false_uncertain', 'true_false', 'continuous'}, default='true_false_uncertain'**) specifies which off-the-shelf template to use, if any. Three off-the-shelf templates offered: incorrect/uncertain/correct (0/0.5/1), incorrect/correct (0/1), and continuous score (0 to 1). These templates are respectively specified as 'true_false_uncertain', 'true_false', and 'continuous'\n",
    "- `system_prompt` - (**str or None, default=\"You are a helpful assistant.\"**) Optional argument for user to provide custom system prompt\n",
    "- `template_ques_ans` - (**str, default=None**) Template for self reflection question, which includes question and answer to compute LLM judge score. Use this to define the LLM response format, if required update argument \"keywords_to_scores_dict\" accordingly. Must be formatted so that template_ques_ans.format(question, answer) places question and answer appropriately in the string. Defaults to variation of Chen et al. (2023).\n",
    "- `keywords_to_scores_dict` - (**dict, default=None**) Keys must be scores, values must be list of strings containing keywords to search. If None, the default dictionary will be used: {\n",
    "    0.0: [\"incorrect\", \"not correct\", \"not right\"],\n",
    "    0.5: [\"not sure\", \"not certain\", \"unsure\", \"uncertain\"],\n",
    "    1.0: [\"correct\", \"right\"],\n",
    "}\n",
    "\n",
    "**Class Methods:**\n",
    "1. `judge_responses` - Generate responses and evaluate confidence scores on LLM responses for the provided prompts..\n",
    "\n",
    "    **Method Attributes:**\n",
    "    - `prompts` - (**list of str**) A list of input prompts for the model.\n",
    "    - `original_responses` - (**list of str, default=None**) A list of model responses for the provided prompts.\n",
    "    - `retries` - (**int, default=5**) Number of times to retry for failed score extraction\n",
    "\n",
    "    **Returns:**\n",
    "    Dictionary containing Q/A concatenation prompts, judge responses, and judge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: We opt to aggregate several judges into a panel of judges using `PanelQuantifier` below.\n",
    "# However, individual judge instances may also be used to directly conduct hallucination evaluation.\n",
    "\n",
    "self_judge = LLMJudge(\n",
    "    llm=original_llm, max_calls_per_min=250, scoring_template=\"true_false\"\n",
    ")\n",
    "judge1 = LLMJudge(\n",
    "    llm=gemini_pro, max_calls_per_min=250, scoring_template=\"true_false_uncertain\"\n",
    ")\n",
    "# judge2 = LLMJudge(langchain_llm=gemini_flash, max_calls_per_min=250, scoring_template='continuous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `LLMPanel()` - Class for aggregating multiple instances of LLMJudge using average, min, max, or majority voting\n",
    "\n",
    "**Class Attributes:**\n",
    "- `judges` - (**list of LLMJudge or BaseChatModel**) Judges to use. If BaseChatModel, LLMJudge is instantiated using default parameters.\n",
    "- `llm` (**BaseChatModel, default=None**) A langchain llm object to get passed to chain constructor. User is responsible for specifying temperature and other relevant parameters to the constructor of their `langchain_llm` object.\n",
    "- `max_calls_per_min` - (**int, default=None**) Specifies number of API calls to make per minute. Used to control rate limiting. Will be used for original langchain_llm and any judges constructed from instances of BaseChatModel in judges\n",
    "- `system_prompt` - (**str or None, default=\"You are a helpful assistant.\"**) Optional argument for user to provide custom system prompt for the original LLM\n",
    "\n",
    "**Class Methods:**\n",
    "1. `evaluate` - Generate responses (if not provided) and use panel to of judges to score responses for correctness.\n",
    "\n",
    "    **Method Attributes:**\n",
    "    - `prompts` - (**list of str**) A list of input prompts for the model.\n",
    "    - `original_responses` - (**list of str, default=None**)  A list of model responses for the provided prompts. If None, responses will be generated using `self.langchain_llm`\n",
    "\n",
    "    **Returns:**\n",
    "       UQResult containing Q/A concatenation prompts, judge responses, and judge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "panel = LLMPanel(\n",
    "    llm=original_llm,\n",
    "    judges=[\n",
    "        self_judge,  # uses same LLM as a judge\n",
    "        judge1,  # customized template (continuous)\n",
    "        gemini_flash,  # constructs directly from BaseChatModel using default template\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses...\n",
      "Generating LLMJudge scores...\n",
      "Generating LLMJudge scores...\n",
      "Generating LLMJudge scores...\n"
     ]
    }
   ],
   "source": [
    "result = await panel.evaluate(\n",
    "    prompts=prompts,\n",
    "    # responses=responses # provide if responses already generated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_2</th>\n",
       "      <th>judge_3</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response  judge_1  \\\n",
       "0  When you solve this math problem only return t...      145      1.0   \n",
       "1  When you solve this math problem only return t...       19      1.0   \n",
       "2  When you solve this math problem only return t...        3      1.0   \n",
       "3  When you solve this math problem only return t...      198      0.0   \n",
       "4  When you solve this math problem only return t...       63      1.0   \n",
       "\n",
       "   judge_2  judge_3       avg  max  min  median  \n",
       "0      0.0      1.0  0.666667  1.0  0.0     1.0  \n",
       "1      1.0      1.0  1.000000  1.0  1.0     1.0  \n",
       "2      1.0      1.0  1.000000  1.0  1.0     1.0  \n",
       "3      0.0      1.0  0.333333  1.0  0.0     0.0  \n",
       "4      1.0      1.0  1.000000  1.0  1.0     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_df()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_2</th>\n",
       "      <th>judge_3</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>answer</th>\n",
       "      <th>response_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response  judge_1  \\\n",
       "0  When you solve this math problem only return t...      145      1.0   \n",
       "1  When you solve this math problem only return t...       19      1.0   \n",
       "2  When you solve this math problem only return t...        3      1.0   \n",
       "3  When you solve this math problem only return t...      198      0.0   \n",
       "4  When you solve this math problem only return t...       63      1.0   \n",
       "\n",
       "   judge_2  judge_3       avg  max  min  median answer  response_correct  \n",
       "0      0.0      1.0  0.666667  1.0  0.0     1.0    145              True  \n",
       "1      1.0      1.0  1.000000  1.0  1.0     1.0     19              True  \n",
       "2      1.0      1.0  1.000000  1.0  1.0     1.0      3              True  \n",
       "3      0.0      1.0  0.333333  1.0  0.0     0.0    198              True  \n",
       "4      1.0      1.0  1.000000  1.0  1.0     1.0     63              True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate correct answers and grade responses\n",
    "result_df[\"answer\"] = svamp.answer\n",
    "result_df[\"response_correct\"] = [\n",
    "    math_postprocessor(r) == a for r, a in zip(result_df[\"response\"], svamp[\"answer\"])\n",
    "]\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge 1 precision: 0.6875\n",
      "Judge 1 recall: 0.7857142857142857\n",
      "Judge 1 f1-score: 0.7333333333333333\n",
      " \n",
      "Judge 2 precision: 0.875\n",
      "Judge 2 recall: 0.5\n",
      "Judge 2 f1-score: 0.6363636363636364\n",
      " \n",
      "Judge 3 precision: 0.9230769230769231\n",
      "Judge 3 recall: 0.8571428571428571\n",
      "Judge 3 f1-score: 0.8888888888888888\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# evaluate precision, recall, and f1-score of Semantic Entropy's predictions of correctness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for ind in [1, 2, 3]:\n",
    "    y_pred = [(s > 0) * 1 for s in result_df[f\"judge_{str(ind)}\"]]\n",
    "    y_true = result_df.response_correct\n",
    "    print(f\"Judge {ind} precision: {precision_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(f\"Judge {ind} recall: {recall_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(f\"Judge {ind} f1-score: {f1_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "uqlm",
   "language": "python",
   "name": "uqlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
